# model_evaluation.ipynb
#!/usr/bin/python3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# 1. Load and preprocess

df = pd.read_csv("cleaned_data.csv")
df["Label"]=df["Label"].apply(lambda val:"ATTACK" if val !="BENIGN" else val)

# Ensure label is binary (0=Benign, 1=Attack)

df['Label'] = df['Label'].map({'BENIGN': 0, 'ATTACK': 1})

X = df.drop('Label', axis=1)
y = df['Label']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

#  2. Define models 
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(probability=True, random_state=42)
}

# 3. Train, evaluate

results = []

plt.figure(figsize=(10, 8))

for name, model in models.items():
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # Classification report
    report = classification_report(y_test, y_pred, output_dict=True)
    precision = report['1']['precision']
    recall = report['1']['recall']
    f1 = report['1']['f1-score']

    # ROC Curve & AUC
    y_score = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

    # Overfitting/Underfitting detection
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)
    fit_status = "OK"
    if train_acc - test_acc > 0.1:
        fit_status = "Overfitting"
    elif test_acc - train_acc > 0.1:
        fit_status = "Underfitting"

    results.append({
        "Model": name,
        "Precision": precision,
        "Recall": recall,
        "F1-score": f1,
        "Train Acc": train_acc,
        "Test Acc": test_acc,
        "Fit Status": fit_status
    })

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend()
plt.show()

# =====================
# 4. Model comparison chart
# =====================
results_df = pd.DataFrame(results)
results_df.set_index("Model", inplace=True)

results_df[["Precision", "Recall", "F1-score"]].plot(
    kind='bar', figsize=(10, 6), colormap='viridis'
)
plt.title("Model Comparison - Precision, Recall, F1")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.show()

# =====================
# 5. Save results
# =====================
results_df.to_csv("model_comparison_results.csv")
print(results_df)
